{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Versions\n",
    " - ClassEffb0_001<br>\n",
    " Baseline Model<br>\n",
    " - ClassEffb0_002<br>\n",
    " 16 Fold, Weight 1-1 Pos-Neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, cv2, time, random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "import albumentations as albu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 16\n",
    "SEED = 1234\n",
    "FOLD = 0\n",
    "CLOUD_TYPE = 'Fish'\n",
    "VERSION = f'ClassEffpseudo002_{CLOUD_TYPE}_{FOLD}'\n",
    "PREVIOUS_VERSION = 'initial'\n",
    "PATH = '../input'\n",
    "TRAIN_PATH = '../input/train_images_525/train_images_525'\n",
    "TEST_PATH = '../input/test_images_525/test_images_525' \n",
    "TRAIN_MASK_PATH = '../input/train_masks_525/train_masks_525'\n",
    "WEIGHT_PATH = '../input/weights'\n",
    "SUBMISSION_PATH = '../input/submissions'\n",
    "N_JOBS = 7\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "IMAGE_SIZE = (1400, 2100)\n",
    "IMAGE_SIZE_525 = (350, 525)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH+f'/pseudo_{CLOUD_TYPE}_{PREVIOUS_VERSION}.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(PATH+f'/pseudo_test_{CLOUD_TYPE}_{PREVIOUS_VERSION}.csv')\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = train_df.index.values\n",
    "\n",
    "train_idx, val_idx = train_test_split(ids, test_size=1/N_SPLITS, random_state=SEED)\n",
    "\n",
    "train_paths = np.array(train_df['path'].iloc[train_idx])\n",
    "val_paths = np.array(train_df['path'].iloc[val_idx])\n",
    "test_paths = sub_df[\"path\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.OneOf([\n",
    "           albu.Resize(320, 480),\n",
    "           albu.RandomCrop(height=320, width=480),\n",
    "           ], p=1),\n",
    "        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.1),\n",
    "        albu.Rotate(limit=20,p=0.3),\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.3, border_mode=0),\n",
    "        albu.GridDistortion(p=0.1),\n",
    "#         albu.OpticalDistortion(p=0.1, distort_limit=2, shift_limit=0.5),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(320, 480),\n",
    "        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def get_test_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(320, 480),\n",
    "        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame = None,\n",
    "        datatype: str = \"train\",\n",
    "        img_ids: np.array = None,\n",
    "        transforms = transforms,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.datatype = datatype\n",
    "        self.img_ids = img_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.img_ids[idx]\n",
    "        \n",
    "        if self.datatype != \"test\":\n",
    "            y = self._find_target(image_path)\n",
    "            y = torch.Tensor(y)\n",
    "        \n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        augmented = self.transforms(image=img)\n",
    "        img = np.transpose(augmented[\"image\"], [2, 0, 1])\n",
    "        \n",
    "        if self.datatype != \"test\":\n",
    "            return img, y\n",
    "        else:\n",
    "            return img\n",
    "    \n",
    "    def _find_target(self, image_name):\n",
    "        df_target = self.df[self.df['path'] == image_name]\n",
    "        y = df_target['hasMask'].values\n",
    "        return y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = N_JOBS\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CloudDataset(df=train_df,\n",
    "                             datatype=\"train\",\n",
    "                             img_ids=train_paths,\n",
    "                             transforms=get_training_augmentation(),)\n",
    "\n",
    "valid_dataset = CloudDataset(df=train_df,\n",
    "                             datatype=\"valid\",\n",
    "                             img_ids=val_paths,\n",
    "                             transforms=get_validation_augmentation(),)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=num_workers)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "model._fc = nn.Sequential(\n",
    "    nn.Linear(in_features=1280, out_features=1, bias=True),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "if train_on_gpu:\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "# optimizer = RAdam(model.parameters(), lr=0.04)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, cooldown=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "train_loss_list = []\n",
    "train_roc_auc_list = []\n",
    "valid_loss_list = []\n",
    "valid_roc_auc_list = []\n",
    "\n",
    "lr_rate_list = []\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "batch_multiplier = 8\n",
    "model.to(DEVICE)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_roc_auc = 0.0\n",
    "    valid_roc_auc = 0.0\n",
    "    count = 0  #multiple minibatch\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    bar = tqdm(train_loader, postfix={\"train_loss\":0.0,\"train_roc_auc\":0.0})\n",
    "    for data, target in bar:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        if count == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = batch_multiplier\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model   \n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target) / batch_multiplier\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        #optimizer.step()\n",
    "        \n",
    "        if count == batch_multiplier:\n",
    "            try:\n",
    "                roc_auc_    = roc_auc_score(targets, outputs)\n",
    "                train_roc_auc += roc_auc_ * data.size(0) * batch_multiplier\n",
    "            except:\n",
    "                pass\n",
    "            targets = target.to(\"cpu\").data.numpy()\n",
    "            outputs = output.to(\"cpu\").data.numpy()\n",
    "        \n",
    "        targets = np.vstack([targets, target.to(\"cpu\").data.numpy()])\n",
    "        outputs = np.vstack([outputs, output.to(\"cpu\").data.numpy()])\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * data.size(0) * batch_multiplier\n",
    "        \n",
    "        count -= 1  #multiple minibatch\n",
    "\n",
    "        try:\n",
    "            bar.set_postfix(ordered_dict={\"train_loss\":loss.item() * batch_multiplier, \"train_roc_auc\":roc_auc_})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    \n",
    "    del data, target\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        bar = tqdm(valid_loader, postfix={\"valid_loss\":0.0, \"roc_auc_score\":0.0})\n",
    "        count = batch_multiplier\n",
    "        \n",
    "        for data, target in bar:\n",
    "            \n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            if count == 0:\n",
    "                count = batch_multiplier\n",
    "                \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            if count == batch_multiplier:\n",
    "                try:\n",
    "                    roc_auc_    = roc_auc_score(targets, outputs)\n",
    "                    valid_roc_auc += roc_auc_ * data.size(0) * batch_multiplier\n",
    "                except:\n",
    "                    pass\n",
    "                targets = target.to(\"cpu\").data.numpy()\n",
    "                outputs = output.to(\"cpu\").data.numpy()\n",
    "                \n",
    "            targets = np.vstack([targets, target.to(\"cpu\").data.numpy()])\n",
    "            outputs = np.vstack([outputs, output.to(\"cpu\").data.numpy()])\n",
    "            \n",
    "            try:\n",
    "                bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"roc_auc_score\":roc_auc_})\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            count -= 1\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    train_roc_auc = train_roc_auc/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    valid_roc_auc = valid_roc_auc/len(valid_loader.dataset)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    train_roc_auc_list.append(train_roc_auc)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    valid_roc_auc_list.append(valid_roc_auc)\n",
    "\n",
    "    lr_rate_list.append([param_group['lr'] for param_group in optimizer.param_groups])\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} || Training Loss: {:.6f}  Training Auc Roc Score: {:.6f} || Validation Loss: {:.6f} Auc Roc Score: {:.6f}'.format(\n",
    "          epoch, train_loss, train_roc_auc, valid_loss, valid_roc_auc))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), f\"{WEIGHT_PATH}/model_{VERSION}_best.pth\")\n",
    "        valid_loss_min = valid_loss\n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([i[0] for i in lr_rate_list])\n",
    "plt.ylabel('learing rate during training', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_loss_list,  marker='o', label=\"Training Loss\")\n",
    "plt.plot(valid_loss_list,  marker='o', label=\"Validation Loss\")\n",
    "plt.ylabel('loss', fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(valid_roc_auc_list)\n",
    "plt.ylabel('Dice score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"{WEIGHT_PATH}/model_{VERSION}_best.pth\"))\n",
    "model.eval()\n",
    "print('Prediction Starts here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pseudo Labelling for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = batch_size\n",
    "test_dataset = CloudDataset(df=sub_df,\n",
    "                            datatype='test', \n",
    "                            img_ids=test_paths,\n",
    "                            transforms=get_test_augmentation())\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    bar = tqdm(test_loader)\n",
    "\n",
    "    for data in bar:\n",
    "        \n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        y_preds.append(output.to(\"cpu\").data.numpy())\n",
    "        \n",
    "y_pred = np.vstack(y_preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_labelling(x):\n",
    "    if x < 0.1:\n",
    "        return 0\n",
    "    elif x > 0.9:\n",
    "        return 1\n",
    "\n",
    "sub_df[f'Pseudo{CLOUD_TYPE}'] = y_pred\n",
    "sub_df['PseudoLabel'] = sub_df[f'Pseudo{CLOUD_TYPE}'].apply(pseudo_labelling)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df = train_df.copy()\n",
    "pseudo_df_test = sub_df.copy()\n",
    "pseudo_df_test_isna = pseudo_df_test[pseudo_df_test['PseudoLabel'].isna()].reset_index(drop=True)\n",
    "pseudo_df_test = pseudo_df_test[pseudo_df_test['PseudoLabel'].notna()].reset_index(drop=True)\n",
    "pseudo_df_test['PseudoLabel'] = pseudo_df_test['PseudoLabel'].astype(int)\n",
    "pseudo_df_test = pseudo_df_test[['path', 'PseudoLabel']].rename(columns={'PseudoLabel':'hasMask'})\n",
    "pseudo_df_test['from'] = 'test'\n",
    "pseudo_df = pd.concat([pseudo_df, pseudo_df_test]).reset_index(drop=True)\n",
    "print('####### Number of Test Data Decreased from {} to {}'.format(sub_df.shape[0], pseudo_df_test_isna.shape[0]))\n",
    "pseudo_df.info()\n",
    "pseudo_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df.to_csv(PATH+f'/pseudo_{CLOUD_TYPE}_{VERSION}.csv', index=False)\n",
    "pseudo_df_test_isna[['path']].to_csv(PATH+f'/pseudo_test_{CLOUD_TYPE}_{VERSION}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
